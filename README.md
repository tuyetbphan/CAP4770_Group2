# CAP4770 Intro. to Data Science Summer 2023
## Group 2 Final Project

**Synopsis of the goal of the project**: 
The success of a business significantly depends on its customers. It is vital for the company to retain its current customers in order to increase profits because acquiring new customers is more expensive. Therefore, it is crucial to gain insight into the factors that can cause a customer to churn or suspend their services with the business. With a dataset from an auto insurance company, we seek to uncover a potential underlying component that makes a customer leave the company. By developing a churn prediction model, we hope this will help in reducing customer churn rate. For this project, we plan to use our knowledge of classification to learn and understand the current customer churn rate for an auto insurance company. Understanding the number of customers that could leave in future months and understanding the reason why could help them create promotions to help retain their existing customers. 

**Details of the dataset**: 
We obtained the dataset from The Open Source Data Project [1] which created the auto insurance dataset for public use, aiming to make it easier for people to explore and analyze data for data science purposes. The dataset is divided into four CSV files with four relational tables and multiple features, each of which contains customer information. The address table has attributes such as latitude, longitude, street address, city, state, and county. The latitude-longitude information mainly references the Dallas-Fort Worth Metroplex in North Texas. The customer table has fields of current annual amount, duration of membership, customer original date, age, date of birth, and social security number. The demographic table contains variables such as income, length of residence, marital status, child status, residential status, property value, higher education status, and credit rating. Lastly, the termination table has the date of the account suspension. Each datapoint in the four tables are connected by the unique customer ID or address ID. The dataset consists of 1,536,673 unique addresses and 2,280,321 unique customers. Among the total number of customers, 2,112,579 have demographic information available, and 269,259 customers canceled their policies in the previous year.

**Evaluation metric and baseline techniques to be used**: 
To evaluate our classifiers, we will use several metrics including accuracy, precision, recall, and F-Score. To calculate these metrics, one needs to understand the definitions of True Positive (TP), True Negative(TN), False Positive (FP), and False Negative (FN). TP and TN are when both the actual class and predicted class match. FP is when the predicted class identifies as positive but the actual is negative, while FN is the opposite. 
The accuracy of a classifier tells us the percentage of correctly identified tuples in the test data set.The precision will tell us how well the classifier labeled actual positive tuples, or how exact the classifier is. Recall helps us understand how accurately were all of the positives identified, if all of the positives were identified correctly we would get a recall of 1, we accurately predicted all positive tuples. It is important to note that there is an inverse relationship between precision and recall. In addition, it is normally very difficult to calculate recall as we would need to know the entire population, to get an accurate false negative count. The F-score metric is the harmonic mean of precision and recall. It will help us understand how accurate the classifier is. The highest possible F-Score is 1, which reflects perfect recall and precision. While the lowest possible F-Score is 0, usually indicated when precision or recall is 0. 
In addition, we will use ROC curves to help us select which of our models performs better. ROC (Receiver Operating Characteristics) curves are visual comparisons of classification models which allow us to see the tradeoff between true positive rate and false positive rate. In order to calculate this accurately, we will need to rank the test tuples from most likely to belong to the positive class to least likely. Once the ROC curves are graphed, we want to calculate the area under the curve which will tell us how accurate the model is. We do not want the model to be 0.5, or close to the diagonal line y=x, since the closer to the ROC curve is to that line, the less accurate the model is. A model with perfect accuracy would have an area of 1 under the ROC curve. 

**Running the code**: 
The Auto_Insurance_Classifier-Final.ipynb is our final version of our code. In order to run the code, please ensure you have the packages listed in teh first kernel installed/up to date. Within our .ipynb file, we have the data preprocessing and cleaning, the three approaches (the three versions of the dataset - Original Preprocessed, Oversampling, and Undersampling) using the Decision Tree and Random Forest Classification models. Followed by the ROC curve and AUC values for all six datasets and models. In addition, we also added a logistic regression information based off the Oversampling data set. 
